
@misc{tang_generalization_2025,
	title = {Generalization {Analysis} for {Bayesian} {Optimal} {Experiment} {Design} under {Model} {Misspecification}},
	url = {http://arxiv.org/abs/2506.07805},
	doi = {10.48550/arXiv.2506.07805},
	abstract = {In many settings in science and industry, such as drug discovery and clinical trials, a central challenge is designing experiments under time and budget constraints. Bayesian Optimal Experimental Design (BOED) is a paradigm to pick maximally informative designs that has been increasingly applied to such problems. During training, BOED selects inputs according to a pre-determined acquisition criterion. During testing, the model learned during training encounters a naturally occurring distribution of test samples. This leads to an instance of covariate shift, where the train and test samples are drawn from different distributions. Prior work has shown that in the presence of model misspecification, covariate shift amplifies generalization error. Our first contribution is to provide a mathematical decomposition of generalization error that reveals key contributors to generalization error in the presence of model misspecification. We show that generalization error under misspecification is the result of, in addition to covariate shift, a phenomenon we term error (de-)amplification which has not been identified or studied in prior work. Our second contribution is to provide a detailed empirical analysis to show that methods that result in representative and de-amplifying training data increase generalization performance. Our third contribution is to develop a novel acquisition function that mitigates the effects of model misspecification by including a term for representativeness and implicitly inducing de-amplification. Our experimental results demonstrate that our method outperforms traditional BOED in the presence of misspecification.},
	urldate = {2025-11-26},
	publisher = {arXiv},
	author = {Tang, Roubing and Sloman, Sabina J. and Kaski, Samuel},
	month = jun,
	year = {2025},
	note = {arXiv:2506.07805 [stat]
version: 1},
	keywords = {Computer Science - Information Theory, Statistics - Machine Learning},
}

@inproceedings{foster_deep_2021,
	title = {Deep {Adaptive} {Design}: {Amortizing} {Sequential} {Bayesian} {Experimental} {Design}},
	shorttitle = {Deep {Adaptive} {Design}},
	url = {https://proceedings.mlr.press/v139/foster21a.html},
	abstract = {We introduce Deep Adaptive Design (DAD), a method for amortizing the cost of adaptive Bayesian experimental design that allows experiments to be run in real-time. Traditional sequential Bayesian optimal experimental design approaches require substantial computation at each stage of the experiment. This makes them unsuitable for most real-world applications, where decisions must typically be made quickly. DAD addresses this restriction by learning an amortized design network upfront and then using this to rapidly run (multiple) adaptive experiments at deployment time. This network represents a design policy which takes as input the data from previous steps, and outputs the next design using a single forward pass; these design decisions can be made in milliseconds during the live experiment. To train the network, we introduce contrastive information bounds that are suitable objectives for the sequential setting, and propose a customized network architecture that exploits key symmetries. We demonstrate that DAD successfully amortizes the process of experimental design, outperforming alternative strategies on a number of problems.},
	language = {en},
	urldate = {2025-11-26},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Foster, Adam and Ivanova, Desi R. and Malik, Ilyas and Rainforth, Tom},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {3384--3395},
}

@article{amortila_mitigating_nodate,
	title = {Mitigating {Covariate} {Shift} in {Misspeciﬁed} {Regression} with {Applications} to {Reinforcement} {Learning}},
	abstract = {A pervasive phenomenon in machine learning applications is distribution shift, where training and deployment conditions for a machine learning model differ. As distribution shift typically results in a degradation in performance, much attention has been devoted to algorithmic interventions that mitigate these detrimental effects. This paper studies the effect of distribution shift in the presence of model misspeciﬁcation, speciﬁcally focusing on L∞-misspeciﬁed regression and adversarial covariate shift, where the regression target remains ﬁxed while the covariate distribution changes arbitrarily. We show that empirical risk minimization, or standard least squares regression, can result in undesirable misspeciﬁcation ampliﬁcation where the error due to misspeciﬁcation is ampliﬁed by the density ratio between the training and testing distributions. As our main result, we develop a new algorithm—inspired by robust optimization techniques—that avoids this undesirable behavior, resulting in no misspeciﬁcation ampliﬁcation while still obtaining optimal statistical rates. As applications, we use this regression procedure to obtain new guarantees in ofﬂine and online reinforcement learning with misspeciﬁcation and establish new separations between previously studied structural conditions and notions of coverage.},
	language = {en},
	author = {Amortila, Philip and Cao, Tongyi and Krishnamurthy, Akshay},
}

@article{huan_optimal_2024,
	title = {Optimal experimental design: {Formulations} and computations},
	volume = {33},
	issn = {0962-4929, 1474-0508},
	shorttitle = {Optimal experimental design},
	url = {https://www.cambridge.org/core/journals/acta-numerica/article/optimal-experimental-design-formulations-and-computations/38BBD0DC1A0386FDF306B6C0167DF7D9},
	doi = {10.1017/S0962492924000023},
	abstract = {Questions of ‘how best to acquire data’ are essential to modelling and prediction in the natural and social sciences, engineering applications, and beyond. Optimal experimental design (OED) formalizes these questions and creates computational methods to answer them. This article presents a systematic survey of modern OED, from its foundations in classical design theory to current research involving OED for complex models. We begin by reviewing criteria used to formulate an OED problem and thus to encode the goal of performing an experiment. We emphasize the flexibility of the Bayesian and decision-theoretic approach, which encompasses information-based criteria that are well-suited to nonlinear and non-Gaussian statistical models. We then discuss methods for estimating or bounding the values of these design criteria; this endeavour can be quite challenging due to strong nonlinearities, high parameter dimension, large per-sample costs, or settings where the model is implicit. A complementary set of computational issues involves optimization methods used to find a design; we discuss such methods in the discrete (combinatorial) setting of observation selection and in settings where an exact design can be continuously parametrized. Finally we present emerging methods for sequential OED that build non-myopic design policies, rather than explicit designs; these methods naturally adapt to the outcomes of past experiments in proposing new experiments, while seeking coordination among all experiments to be performed. Throughout, we highlight important open questions and challenges.},
	language = {en},
	urldate = {2025-11-21},
	journal = {Acta Numerica},
	author = {Huan, Xun and Jagalur, Jayanth and Marzouk, Youssef},
	month = jul,
	year = {2024},
	keywords = {62-02, 62-08, 62B15, 62K05, 62L05, 65M32, 94A17},
	pages = {715--840},
}

@inproceedings{smith_prediction-oriented_2023,
	title = {Prediction-{Oriented} {Bayesian} {Active} {Learning}},
	url = {https://proceedings.mlr.press/v206/bickfordsmith23a.html},
	abstract = {Information-theoretic approaches to active learning have traditionally focused on maximising the information gathered about the model parameters, most commonly by optimising the BALD score. We highlight that this can be suboptimal from the perspective of predictive performance. For example, BALD lacks a notion of an input distribution and so is prone to prioritise data of limited relevance. To address this we propose the expected predictive information gain (EPIG), an acquisition function that measures information gain in the space of predictions rather than parameters. We find that using EPIG leads to stronger predictive performance compared with BALD across a range of datasets and models, and thus provides an appealing drop-in replacement.},
	language = {en},
	urldate = {2025-11-21},
	booktitle = {Proceedings of {The} 26th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Smith, Freddie Bickford and Kirsch, Andreas and Farquhar, Sebastian and Gal, Yarin and Foster, Adam and Rainforth, Tom},
	month = apr,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {7331--7348},
}

@phdthesis{feng_optimal_2015,
	type = {Thesis},
	title = {Optimal {Bayesian} experimental design in the presence of model error},
	copyright = {M.I.T. theses are protected by copyright. They may be viewed from this source for any purpose, but reproduction or distribution in any format is prohibited without written permission. See provided URL for inquiries about permission.},
	url = {https://dspace.mit.edu/handle/1721.1/97790},
	abstract = {The optimal selection of experimental conditions is essential to maximizing the value of data for inference and prediction. We propose an information theoretic framework and algorithms for robust optimal experimental design with simulation-based models, with the goal of maximizing information gain in targeted subsets of model parameters, particularly in situations where experiments are costly. Our framework employs a Bayesian statistical setting, which naturally incorporates heterogeneous sources of information. An objective function reflects expected information gain from proposed experimental designs. Monte Carlo sampling is used to evaluate the expected information gain, and stochastic approximation algorithms make optimization feasible for computationally intensive and high-dimensional problems. A key aspect of our framework is the introduction of model calibration discrepancy terms that are used to "relax" the model so that proposed optimal experiments are more robust to model error or inadequacy. We illustrate the approach via several model problems and misspecification scenarios. In particular, we show how optimal designs are modified by allowing for model error, and we evaluate the performance of various designs by simulating "real-world" data from models not considered explicitly in the optimization objective.},
	language = {eng},
	urldate = {2025-11-21},
	school = {Massachusetts Institute of Technology},
	author = {Feng, Chi},
	year = {2015},
	note = {Accepted: 2015-07-17T19:46:48Z},
}

@inproceedings{go_robust_2022,
	title = {Robust expected information gain for optimal {Bayesian} experimental design using ambiguity sets},
	url = {https://proceedings.mlr.press/v180/go22a.html},
	abstract = {The ranking of experiments by expected information gain (EIG) in Bayesian experimental design is sensitive to changes in the model’s prior distribution, and the approximation of EIG yielded by sampling will have errors similar to the use of a perturbed prior. We define and analyze Robust Expected Information Gain(REIG), a modification of the objective in EIG maximization by minimizing an affine relaxation of EIG over an ambiguity set of distributions that are close to the original prior in KL-divergence. We show that, when combined with a sampling-based approach to estimating EIG, REIG corresponds to a "log-sum-exp" stabilization of the samples used to estimate EIG, meaning that it can be efficiently implemented in practice. Numerical tests combining REIG with variational nested Monte Carlo (VNMC), adaptive contrastive estimation (ACE) and mutual information neural estimation (MINE) suggest that in practice REIG also compensates for the variability of under-sampled estimators.},
	language = {en},
	urldate = {2025-11-21},
	booktitle = {Proceedings of the {Thirty}-{Eighth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {PMLR},
	author = {Go, Jinwoo and Isaac, Tobin},
	month = aug,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {728--737},
}

@article{overstall_bayesian_2022,
	title = {Bayesian {Decision}-{Theoretic} {Design} of {Experiments} {Under} an {Alternative} {Model}},
	volume = {17},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-17/issue-4/Bayesian-Decision-Theoretic-Design-of-Experiments-Under-an-Alternative-Model/10.1214/21-BA1286.full},
	doi = {10.1214/21-BA1286},
	abstract = {Traditionally Bayesian decision-theoretic design of experiments proceeds by choosing a design to minimise expectation of a given loss function over the space of all designs. The loss function encapsulates the aim of the experiment, and the expectation is taken with respect to the joint distribution of all unknown quantities implied by the statistical model that will be fitted to observed responses. In this paper, an extended framework is proposed whereby the expectation of the loss is taken with respect to a joint distribution implied by an alternative statistical model. Motivation for this includes promoting robustness, ensuring computational feasibility and for allowing realistic prior specification when deriving a design. To aid in exploring the new framework, an asymptotic approximation to the expected loss under an alternative model is derived, and the properties of different loss functions are established. The framework is then demonstrated on a linear regression versus full-treatment model scenario, on estimating parameters of a non-linear model under model discrepancy and a cubic spline model under an unknown number of basis functions.},
	number = {4},
	urldate = {2025-11-21},
	journal = {Bayesian Analysis},
	author = {Overstall, Antony and McGree, James},
	month = dec,
	year = {2022},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {Non-linear model, cubic spline basis, expected loss function, full-treatment model, model discrepancy, normal linear model},
	pages = {1021--1041},
}

@inproceedings{forster_improving_2025,
	title = {Improving {Robustness} to {Model} {Misspecification} in {Bayesian} {Experimental} {Design}},
	url = {https://openreview.net/forum?id=uyNJLdCvFG},
	abstract = {We propose a method to improve robustness to model misspecification in Bayesian experimental design (BED). Our approach introduces a flexible auxiliary model and jointly optimizes the expected information gain (EIG) in the original model parameters, the predictions of the auxiliary model, and a Bernoulli random variable indicating whether the original model is correct or misspecified. We show this balances learning about the original model, gathering data useful for general prediction, and assessing model fit. By leveraging the domain-specific knowledge embedded in the original model, we guide the design process while maintaining flexibility in the face of model misspecification. This is particularly important in adaptive design settings, where the original model informs early design decisions, but the auxiliary model enables adaptation when new data reveals model inaccuracies.},
	language = {en},
	urldate = {2025-11-20},
	author = {Forster, Alex and Ivanova, Desi R. and Rainforth, Tom},
	month = mar,
	year = {2025},
}
