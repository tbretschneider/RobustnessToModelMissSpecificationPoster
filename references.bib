
@misc{houlsby_bayesian_2011,
	title = {Bayesian {Active} {Learning} for {Classification} and {Preference} {Learning}},
	url = {http://arxiv.org/abs/1112.5745},
	doi = {10.48550/arXiv.1112.5745},
	abstract = {Information theoretic active learning has been widely studied for probabilistic models. For simple regression an optimal myopic policy is easily tractable. However, for other tasks and with more complex models, such as classification with nonparametric models, the optimal solution is harder to compute. Current approaches make approximations to achieve tractability. We propose an approach that expresses information gain in terms of predictive entropies, and apply this method to the Gaussian Process Classifier (GPC). Our approach makes minimal approximations to the full information theoretic objective. Our experimental performance compares favourably to many popular active learning algorithms, and has equal or lower computational complexity. We compare well to decision theoretic approaches also, which are privy to more information and require much more computational time. Secondly, by developing further a reformulation of binary preference learning to a classification problem, we extend our algorithm to Gaussian Process preference learning.},
	urldate = {2025-11-27},
	publisher = {arXiv},
	author = {Houlsby, Neil and Huszár, Ferenc and Ghahramani, Zoubin and Lengyel, Máté},
	month = dec,
	year = {2011},
	note = {arXiv:1112.5745 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{catanach_metrics_2023,
	title = {Metrics for {Bayesian} {Optimal} {Experiment} {Design} under {Model} {Misspecification}},
	url = {http://arxiv.org/abs/2304.07949},
	doi = {10.48550/arXiv.2304.07949},
	abstract = {The conventional approach to Bayesian decisiontheoretic experiment design involves searching over possible experiments to select a design that maximizes the expected value of a speciﬁed utility function. The expectation is over the joint distribution of all unknown variables implied by the statistical model that will be used to analyze the collected data. The utility function deﬁnes the objective of the experiment where a common utility function is the information gain. This article introduces an expanded framework for this process, where we go beyond the traditional Expected Information Gain criteria and introduce the Expected General Information Gain which measures robustness to the model discrepancy and Expected Discriminatory Information as a criterion to quantify how well an experiment can detect model discrepancy. The functionality of the framework is showcased through its application to a scenario involving a linearized spring mass damper system and an F-16 model where the model discrepancy is taken into account while doing Bayesian optimal experiment design.},
	language = {en},
	urldate = {2025-11-27},
	publisher = {arXiv},
	author = {Catanach, Tommie A. and Das, Niladri},
	month = apr,
	year = {2023},
	note = {arXiv:2304.07949 [stat]},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Statistics - Machine Learning, Statistics - Methodology},
}

@article{yang_active_2025,
	title = {Active {Learning} of {Model} {Discrepancy} with {Bayesian} {Experimental} {Design}},
	volume = {446},
	issn = {00457825},
	url = {http://arxiv.org/abs/2502.05372},
	doi = {10.1016/j.cma.2025.118198},
	abstract = {Digital twins have been actively explored in many engineering applications, such as manufacturing and autonomous systems. However, model discrepancy is ubiquitous in most digital twin models and has significant impacts on the performance of using those models. In recent years, data-driven modeling techniques have been demonstrated promising in characterizing the model discrepancy in existing models, while the training data for the learning of model discrepancy is often obtained in an empirical way and an active approach of gathering informative data can potentially benefit the learning of model discrepancy. On the other hand, Bayesian experimental design (BED) provides a systematic approach to gathering the most informative data, but its performance is often negatively impacted by the model discrepancy. In this work, we build on sequential BED and propose an efficient approach to iteratively learn the model discrepancy based on the data from the BED. The performance of the proposed method is validated by a classical numerical example governed by a convection-diffusion equation, for which full BED is still feasible. The proposed method is then further studied in the same numerical example with a high-dimensional model discrepancy, which serves as a demonstration for the scenarios where full BED is not practical anymore. In this example with time-varying velocity, the network only learns the stationary discrepancy and leaves the time dependence being handled by the physics-based model itself, which helps mitigate ill-posedness of the joint learning problem. An ensemble-based approximation of information gain is further utilized to assess the data informativeness and to enhance learning model discrepancy. The results show that the proposed method is efficient and robust to the active learning of highdimensional model discrepancy, using data suggested by the sequential BED. We also demonstrate that the proposed method is compatible with both classical numerical solvers and modern autodifferentiable solvers.},
	language = {en},
	urldate = {2025-11-27},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Yang, Huchen and Chen, Chuanqi and Wu, Jin-Long},
	month = nov,
	year = {2025},
	note = {arXiv:2502.05372 [cs]},
	keywords = {Computer Science - Machine Learning},
	pages = {118198},
}

@misc{tang_generalization_2025,
	title = {Generalization {Analysis} for {Bayesian} {Optimal} {Experiment} {Design} under {Model} {Misspecification}},
	url = {http://arxiv.org/abs/2506.07805},
	doi = {10.48550/arXiv.2506.07805},
	abstract = {In many settings in science and industry, such as drug discovery and clinical trials, a central challenge is designing experiments under time and budget constraints. Bayesian Optimal Experimental Design (BOED) is a paradigm to pick maximally informative designs that has been increasingly applied to such problems. During training, BOED selects inputs according to a pre-determined acquisition criterion. During testing, the model learned during training encounters a naturally occurring distribution of test samples. This leads to an instance of covariate shift, where the train and test samples are drawn from different distributions. Prior work has shown that in the presence of model misspecification, covariate shift amplifies generalization error. Our first contribution is to provide a mathematical decomposition of generalization error that reveals key contributors to generalization error in the presence of model misspecification. We show that generalization error under misspecification is the result of, in addition to covariate shift, a phenomenon we term error (de-)amplification which has not been identified or studied in prior work. Our second contribution is to provide a detailed empirical analysis to show that methods that result in representative and de-amplifying training data increase generalization performance. Our third contribution is to develop a novel acquisition function that mitigates the effects of model misspecification by including a term for representativeness and implicitly inducing de-amplification. Our experimental results demonstrate that our method outperforms traditional BOED in the presence of misspecification.},
	urldate = {2025-11-26},
	publisher = {arXiv},
	author = {Tang, Roubing and Sloman, Sabina J. and Kaski, Samuel},
	month = jun,
	year = {2025},
	note = {arXiv:2506.07805 [stat]
version: 1},
	keywords = {Computer Science - Information Theory, Statistics - Machine Learning},
}

@inproceedings{foster_deep_2021,
	title = {Deep {Adaptive} {Design}: {Amortizing} {Sequential} {Bayesian} {Experimental} {Design}},
	shorttitle = {Deep {Adaptive} {Design}},
	url = {https://proceedings.mlr.press/v139/foster21a.html},
	abstract = {We introduce Deep Adaptive Design (DAD), a method for amortizing the cost of adaptive Bayesian experimental design that allows experiments to be run in real-time. Traditional sequential Bayesian optimal experimental design approaches require substantial computation at each stage of the experiment. This makes them unsuitable for most real-world applications, where decisions must typically be made quickly. DAD addresses this restriction by learning an amortized design network upfront and then using this to rapidly run (multiple) adaptive experiments at deployment time. This network represents a design policy which takes as input the data from previous steps, and outputs the next design using a single forward pass; these design decisions can be made in milliseconds during the live experiment. To train the network, we introduce contrastive information bounds that are suitable objectives for the sequential setting, and propose a customized network architecture that exploits key symmetries. We demonstrate that DAD successfully amortizes the process of experimental design, outperforming alternative strategies on a number of problems.},
	language = {en},
	urldate = {2025-11-26},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Foster, Adam and Ivanova, Desi R. and Malik, Ilyas and Rainforth, Tom},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {3384--3395},
}

@article{amortila_mitigating_nodate,
	title = {Mitigating {Covariate} {Shift} in {Misspeciﬁed} {Regression} with {Applications} to {Reinforcement} {Learning}},
	abstract = {A pervasive phenomenon in machine learning applications is distribution shift, where training and deployment conditions for a machine learning model differ. As distribution shift typically results in a degradation in performance, much attention has been devoted to algorithmic interventions that mitigate these detrimental effects. This paper studies the effect of distribution shift in the presence of model misspeciﬁcation, speciﬁcally focusing on L∞-misspeciﬁed regression and adversarial covariate shift, where the regression target remains ﬁxed while the covariate distribution changes arbitrarily. We show that empirical risk minimization, or standard least squares regression, can result in undesirable misspeciﬁcation ampliﬁcation where the error due to misspeciﬁcation is ampliﬁed by the density ratio between the training and testing distributions. As our main result, we develop a new algorithm—inspired by robust optimization techniques—that avoids this undesirable behavior, resulting in no misspeciﬁcation ampliﬁcation while still obtaining optimal statistical rates. As applications, we use this regression procedure to obtain new guarantees in ofﬂine and online reinforcement learning with misspeciﬁcation and establish new separations between previously studied structural conditions and notions of coverage.},
	language = {en},
	author = {Amortila, Philip and Cao, Tongyi and Krishnamurthy, Akshay},
}

@article{huan_optimal_2024,
	title = {Optimal experimental design: {Formulations} and computations},
	volume = {33},
	issn = {0962-4929, 1474-0508},
	shorttitle = {Optimal experimental design},
	url = {https://www.cambridge.org/core/journals/acta-numerica/article/optimal-experimental-design-formulations-and-computations/38BBD0DC1A0386FDF306B6C0167DF7D9},
	doi = {10.1017/S0962492924000023},
	abstract = {Questions of ‘how best to acquire data’ are essential to modelling and prediction in the natural and social sciences, engineering applications, and beyond. Optimal experimental design (OED) formalizes these questions and creates computational methods to answer them. This article presents a systematic survey of modern OED, from its foundations in classical design theory to current research involving OED for complex models. We begin by reviewing criteria used to formulate an OED problem and thus to encode the goal of performing an experiment. We emphasize the flexibility of the Bayesian and decision-theoretic approach, which encompasses information-based criteria that are well-suited to nonlinear and non-Gaussian statistical models. We then discuss methods for estimating or bounding the values of these design criteria; this endeavour can be quite challenging due to strong nonlinearities, high parameter dimension, large per-sample costs, or settings where the model is implicit. A complementary set of computational issues involves optimization methods used to find a design; we discuss such methods in the discrete (combinatorial) setting of observation selection and in settings where an exact design can be continuously parametrized. Finally we present emerging methods for sequential OED that build non-myopic design policies, rather than explicit designs; these methods naturally adapt to the outcomes of past experiments in proposing new experiments, while seeking coordination among all experiments to be performed. Throughout, we highlight important open questions and challenges.},
	language = {en},
	urldate = {2025-11-21},
	journal = {Acta Numerica},
	author = {Huan, Xun and Jagalur, Jayanth and Marzouk, Youssef},
	month = jul,
	year = {2024},
	keywords = {62-02, 62-08, 62B15, 62K05, 62L05, 65M32, 94A17},
	pages = {715--840},
}

@inproceedings{smith_prediction-oriented_2023,
	title = {Prediction-{Oriented} {Bayesian} {Active} {Learning}},
	url = {https://proceedings.mlr.press/v206/bickfordsmith23a.html},
	abstract = {Information-theoretic approaches to active learning have traditionally focused on maximising the information gathered about the model parameters, most commonly by optimising the BALD score. We highlight that this can be suboptimal from the perspective of predictive performance. For example, BALD lacks a notion of an input distribution and so is prone to prioritise data of limited relevance. To address this we propose the expected predictive information gain (EPIG), an acquisition function that measures information gain in the space of predictions rather than parameters. We find that using EPIG leads to stronger predictive performance compared with BALD across a range of datasets and models, and thus provides an appealing drop-in replacement.},
	language = {en},
	urldate = {2025-11-21},
	booktitle = {Proceedings of {The} 26th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Smith, Freddie Bickford and Kirsch, Andreas and Farquhar, Sebastian and Gal, Yarin and Foster, Adam and Rainforth, Tom},
	month = apr,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {7331--7348},
}

@phdthesis{feng_optimal_2015,
	type = {Thesis},
	title = {Optimal {Bayesian} experimental design in the presence of model error},
	copyright = {M.I.T. theses are protected by copyright. They may be viewed from this source for any purpose, but reproduction or distribution in any format is prohibited without written permission. See provided URL for inquiries about permission.},
	url = {https://dspace.mit.edu/handle/1721.1/97790},
	abstract = {The optimal selection of experimental conditions is essential to maximizing the value of data for inference and prediction. We propose an information theoretic framework and algorithms for robust optimal experimental design with simulation-based models, with the goal of maximizing information gain in targeted subsets of model parameters, particularly in situations where experiments are costly. Our framework employs a Bayesian statistical setting, which naturally incorporates heterogeneous sources of information. An objective function reflects expected information gain from proposed experimental designs. Monte Carlo sampling is used to evaluate the expected information gain, and stochastic approximation algorithms make optimization feasible for computationally intensive and high-dimensional problems. A key aspect of our framework is the introduction of model calibration discrepancy terms that are used to "relax" the model so that proposed optimal experiments are more robust to model error or inadequacy. We illustrate the approach via several model problems and misspecification scenarios. In particular, we show how optimal designs are modified by allowing for model error, and we evaluate the performance of various designs by simulating "real-world" data from models not considered explicitly in the optimization objective.},
	language = {eng},
	urldate = {2025-11-21},
	school = {Massachusetts Institute of Technology},
	author = {Feng, Chi},
	year = {2015},
	note = {Accepted: 2015-07-17T19:46:48Z},
}

@inproceedings{go_robust_2022,
	title = {Robust expected information gain for optimal {Bayesian} experimental design using ambiguity sets},
	url = {https://proceedings.mlr.press/v180/go22a.html},
	abstract = {The ranking of experiments by expected information gain (EIG) in Bayesian experimental design is sensitive to changes in the model’s prior distribution, and the approximation of EIG yielded by sampling will have errors similar to the use of a perturbed prior. We define and analyze Robust Expected Information Gain(REIG), a modification of the objective in EIG maximization by minimizing an affine relaxation of EIG over an ambiguity set of distributions that are close to the original prior in KL-divergence. We show that, when combined with a sampling-based approach to estimating EIG, REIG corresponds to a "log-sum-exp" stabilization of the samples used to estimate EIG, meaning that it can be efficiently implemented in practice. Numerical tests combining REIG with variational nested Monte Carlo (VNMC), adaptive contrastive estimation (ACE) and mutual information neural estimation (MINE) suggest that in practice REIG also compensates for the variability of under-sampled estimators.},
	language = {en},
	urldate = {2025-11-21},
	booktitle = {Proceedings of the {Thirty}-{Eighth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {PMLR},
	author = {Go, Jinwoo and Isaac, Tobin},
	month = aug,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {728--737},
}

@article{overstall_bayesian_2022,
	title = {Bayesian {Decision}-{Theoretic} {Design} of {Experiments} {Under} an {Alternative} {Model}},
	volume = {17},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-17/issue-4/Bayesian-Decision-Theoretic-Design-of-Experiments-Under-an-Alternative-Model/10.1214/21-BA1286.full},
	doi = {10.1214/21-BA1286},
	abstract = {Traditionally Bayesian decision-theoretic design of experiments proceeds by choosing a design to minimise expectation of a given loss function over the space of all designs. The loss function encapsulates the aim of the experiment, and the expectation is taken with respect to the joint distribution of all unknown quantities implied by the statistical model that will be fitted to observed responses. In this paper, an extended framework is proposed whereby the expectation of the loss is taken with respect to a joint distribution implied by an alternative statistical model. Motivation for this includes promoting robustness, ensuring computational feasibility and for allowing realistic prior specification when deriving a design. To aid in exploring the new framework, an asymptotic approximation to the expected loss under an alternative model is derived, and the properties of different loss functions are established. The framework is then demonstrated on a linear regression versus full-treatment model scenario, on estimating parameters of a non-linear model under model discrepancy and a cubic spline model under an unknown number of basis functions.},
	number = {4},
	urldate = {2025-11-21},
	journal = {Bayesian Analysis},
	author = {Overstall, Antony and McGree, James},
	month = dec,
	year = {2022},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {Non-linear model, cubic spline basis, expected loss function, full-treatment model, model discrepancy, normal linear model},
	pages = {1021--1041},
}

@inproceedings{forster_improving_2025,
	title = {Improving {Robustness} to {Model} {Misspecification} in {Bayesian} {Experimental} {Design}},
	url = {https://openreview.net/forum?id=uyNJLdCvFG},
	abstract = {We propose a method to improve robustness to model misspecification in Bayesian experimental design (BED). Our approach introduces a flexible auxiliary model and jointly optimizes the expected information gain (EIG) in the original model parameters, the predictions of the auxiliary model, and a Bernoulli random variable indicating whether the original model is correct or misspecified. We show this balances learning about the original model, gathering data useful for general prediction, and assessing model fit. By leveraging the domain-specific knowledge embedded in the original model, we guide the design process while maintaining flexibility in the face of model misspecification. This is particularly important in adaptive design settings, where the original model informs early design decisions, but the auxiliary model enables adaptation when new data reveals model inaccuracies.},
	language = {en},
	urldate = {2025-11-20},
	author = {Forster, Alex and Ivanova, Desi R. and Rainforth, Tom},
	month = mar,
	year = {2025},
}
